# ğŸ¤– GenAI DevOps WebProject

> A full-stack Generative AI assistant built with FastAPI, OpenAI, and Next.js â€” containerized using Docker for seamless deployment.

---

## ğŸŒŸ Project Overview

**GenAI Assistant** is a full-stack web application designed to:
- Accept user prompts for **interview questions**, **code explanations**, or **DevOps troubleshooting**
- Forward them to a **FastAPI** backend, which communicates with **OpenAI APIs**
- Display AI-generated responses in a **Next.js + Tailwind CSS** frontend

This project showcases a clean separation of UI, backend logic, and AI integration â€” making it a solid example of a **DevOps-enabled GenAI architecture**.

---

## ğŸ”‚ï¸ File Structure Summary

```
GenAI-Assistant/
â”œâ”€â”€ docker-compose.yml           # Compose setup for full-stack app
â”œâ”€â”€ backend/                     # FastAPI + OpenAI integration
â”‚   â”œâ”€â”€ main.py                  # Entry point for API
â”‚   â”œâ”€â”€ utils/                   # Business logic: interview, code explainer, DevOps bot
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env                     # Contains OPENAI_API_KEY (not committed)
â”œâ”€â”€ frontend/                    # Next.js + Tailwind UI
â”‚   â”œâ”€â”€ pages/                   # Routes: /interview, /explain-code, /devops-assistant
â”‚   â”œâ”€â”€ components/              # Shared: Navbar, Footer, Layout
â”‚   â”œâ”€â”€ public/                  # Assets: logos, icons
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ tailwind.config.js
```

---

## ğŸ§ How It Works

### ğŸ”„ Full Flow

```
USER PROMPT â†’ Frontend (Next.js)
â†’ POST /interview | /explain-code | /devops-assistant â†’ FastAPI (Python)
â†’ OpenAI API (GPT-3.5/4)
â†’ Response â†’ UI Rendered with Typing Animation
```

- API key is securely stored in `.env` (not exposed to frontend)
- Requests are handled via `utils/` modules like:
  - `interview.py`
  - `code_explainer.py`
  - `devops_assistant.py`

---

## ğŸš€ Docker & DevOps Setup

- **Dockerized Full Stack**: Both frontend and backend have individual Dockerfiles
- **Docker Compose** spins everything up:
  - **Backend**: http://localhost:8000
  - **Frontend**: http://localhost:3000
- Internal networking via:
  ```env
  NEXT_PUBLIC_API_BASE_URL=http://backend:8000
  ```

---

## ğŸ’» Available Routes

| Route               | Description                           |
|--------------------|---------------------------------------|
| `/`                | Landing page with animations          |
| `/interview`       | Simulates technical interviews        |
| `/explain-code`    | Explains code using GPT               |
| `/devops-assistant`| Troubleshoots DevOps queries          |

---

## ğŸ§ Why is this a GenAI DevOps Project?

- Uses **OpenAI LLMs** (GPT-3.5/4) to generate intelligent, human-like responses
- Built with a clean CI/CD-ready architecture
- Seamlessly integrates **DevOps tooling** like Docker & environment separation

---

## ğŸ” Secrets & Security

- API keys are managed in `backend/.env`
- Not committed (listed in `.gitignore`)
- Loaded securely using `dotenv` and `os.getenv()`

---

## ğŸ“¸ Screenshots (optional)

Add screenshots or a short demo here to show off the UI and interaction!

![genAI 1](https://github.com/user-attachments/assets/08e14f84-6766-4403-8631-94c1ccf821eb)
![genAI 2](https://github.com/user-attachments/assets/c63823de-a24d-4563-a642-a20d2f155dbc)
![genAI 3](https://github.com/user-attachments/assets/1bf49b7a-81ba-404f-9a1d-da81da300551)
![genAI 4](https://github.com/user-attachments/assets/1ca381ad-afb8-44da-b95d-d5d9e54b8cc0)

---

## ğŸ› ï¸ Tech Stack

| Layer         | Tech Used               |
|---------------|--------------------------|
| UI            | Next.js + Tailwind CSS   |
| Backend API   | FastAPI (Python)         |
| LLM Brain     | OpenAI API (GPT-3.5/4)   |
| Infrastructure| Docker + Docker Compose  |

---

## âœ¨ Get Started

```bash
# Start full-stack app with Docker
docker-compose up --build
```

Then open:
- Frontend â†’ http://localhost:3000
- Backend API â†’ http://localhost:8000


